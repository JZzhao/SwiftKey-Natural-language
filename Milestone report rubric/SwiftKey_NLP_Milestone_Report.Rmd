---
title: "SwiftKey NLP Milestone Report"
author: "Tianxiang(Ivan) Liu"
date: "13 November 2014"
output: html_document
---

```{r echo=FALSE, message=FALSE}
setwd('/Users/ivan/Work_directory/SwiftKey')
require(tm); require(SnowballC); require(data.table)
require(ggplot2); require(RWeka); require(qdap);
require(scales); require(gridExtra); require(wordcloud)
source('SwiftKey-Natural-language/Task_1.5_Tokenization_func.R')
```

### Introduction
This milestone report introduced a preliminary research about SwiftKey's NLP project.

The SwiftKey's NLP project is mainly aimed to implement NLP(Natural language processing) techniques to build an algorithm in R environment. The algorithm will be trained by a larged amounts of collected text/documents and eventually, it will be able to make predictions of words that users are most likely to type. This predictive model will be built up for mobile keyboards, so a tradeoff between model capbility and efficiency will be considered during project.

The word prediction project has been started and an initial assessment of the task completed. This report describes the goals of the overall project, the magnitude of the challenge, and gives some initial assessments of the results.

In this report, the basic research like preprocess of raw data, preliminary statistics/visualization analysis, plans for algorithm and applications will be introduced so that reader can have an overall concept regarding the project and where the project is heading to.

Following flowchart gives us the main steps will be included of SwiftKey's NLP project. 



### Preprocess
The preprocess for text mining mainly includes cleaning, tokenization and stemming. The objectives of these processes are to clean the collections of text documents provided and transfer the documents into a form of text segmentation which can be used for further analysis easily. To be more specific, the following issues in text documents will be solved during preprocess:

1. Capital/Lower case
2. Numbers
3. Punctuations
4. Whitespace
5. Profanity words
6. Special notation/Noise like mistypes, UTF-16 encoded characters, foreign words, etc.

To overcome all issues above, function tokenization() has been constructed and following is an output of applying this function on our documents.
 
```{r echo=FALSE}
docs <- en_US.document[1]
trans <- c(F,T,T,T,F,F,T,T)
ChartoSpace <- c('/','\\|')
stopWords <- 'english'
ownStopWords <- c()
swearwords <- read.table('SwiftKey-Natural-language/profanity filter/en', sep='\n')
names(swearwords)<-'swearwords'
filter <- rep('***', length(swearwords))
profanity <- data.frame(swearwords, target = filter)
profanity <- rbind(profanity, data.frame(swearwords = c("[^[:alpha:][:space:]']","â ","ã","ð"), target = c(" ","'","'","'")))
tokenized_docs <- tokenization(docs, trans, ChartoSpace,
                               stopWords, ownStopWords, profanity)
```

After tokenizations, stemming is applied to documents to remove common words endings for English words, such as "es", "ed" and "s". 
```{r echo=FALSE}
tokenized_docs<-tokenized_docs[regexpr(pattern = '^([a-zA-Z])(?!(\\1{1,}))[a-zA-Z]*([a-zA-Z]+-([a-zA-Z]){2,})?(\'(s)?)?$', tokenized_docs, perl=T )>0]
stem_docs <- tm_map(tokenized_docs, stemDocument, 'english') # SnowballStemmer
df <- data.frame(text=unlist(sapply(tokenized_docs, '[',"content")),stringsAsFactors=F)
```

sample
corpus - data.frame - ngrams

### Preliminary Statistics/Visualization
Line num
Word count
ngram 1-4
words DTM (keep)
correlation diagram
word cloud


### Prediction Algorithm
sparsity


### Application
